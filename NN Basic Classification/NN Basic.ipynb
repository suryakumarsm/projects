{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7por6hHzcgTW"
      },
      "source": [
        "# Neural Network Basic Classification Problem \n",
        "Here the following steps gives a broad overview of data source, getting the data and customizing for Neural Network Classification problem. Then a brief steps about creating a classification model.  \n",
        "1. Read the data dictionary and description from here:\n",
        "\n",
        "* https://oehha.ca.gov/media/downloads/calenviroscreen/document/calenviroscreen40resultsdatadictionaryf2021.zip\n",
        "\n",
        " Download the data using gdown, read the Excel file using pandas, print the first 10 rows using df.head() and use df.info() to examine the data types and missing values.\n",
        "\n",
        "2. Simplify the raw dataframe so that you only keep the columns you need. The `X` variable will be the following columns: `Population`, `Ozone` through `Solid Waste Pctl`, and `Asthma` through `Linguistic Isolation Pctl`. The `y` variable will be `Poverty`. Examine the quality of each column and use your judgement about dropping rows or imputing missing values. Add text cells and lots of comments so we can understand your logic/justification!\n",
        "\n",
        "3. Recode the target variable to a 1 if greater than the mean value of poverty, otherwise make it a 0. Use this recoded variable as the target variable! Now it is a classification problem.\n",
        "\n",
        "4. Make two interesting plots or tables and a description for EDA.\n",
        "\n",
        "5. Do an 90/10 split for X_train, X_test, y_train, y_test.\n",
        "\n",
        "6. Use the StandardScaler() on train and apply to test partition. Do not scale the target variable!\n",
        "\n",
        "7. Build a model using the Sequential API (like we do in class) with at least 2 dense layers with the relu activation function, and with dropout in between each dense layer (use a number between 0.1 and 0.5). Compile the model using an appropriate optimizer. Use early stopping with patience of at least 10 and restore the best weights once the model converges. You can choose whatever batch size you would like to.\n",
        "\n",
        "8. Fit the model for 100000 epochs with a batch size of your choice, using X_test and y_test as the validation data. **Don’t forget the early stopping callback!**\n",
        "\n",
        "9. Evaluate the model using learning curves, error metrics and confusion matrices for each partition.\n",
        "\n",
        "10. Calculate what a baseline prediction would be for the train and test partitions (a mean only model). Did your model do better than the baseline predictions? If so, you have a useful model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSBDw0iEErUJ"
      },
      "source": [
        "#Q1. Reading Data and Importing Modules\n",
        "Read Data and Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gFcKcj142-Yw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\surya\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\surya\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# import modules\n",
        "# for general data analysis/plotting\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# for data prep\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# neural net modules\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# File ID and URL\n",
        "file_id = '1_8vmQwSZ02ZOMw_IPHY5OWkwpXQLVW2E'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the content to a local file\n",
        "with open('CalEnviroScreen.xlsx', 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Read the Excel file into a DataFrame\n",
        "df_CalEnviroScreen = pd.read_excel('CalEnviroScreen.xlsx')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKQYozqn3Drz",
        "outputId": "b006d4be-71bb-41f2-d99a-06a885717e4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'gdown' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'CalEnviroScreen.xlsx'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# https://drive.google.com/file/d/1_8vmQwSZ02ZOMw_IPHY5OWkwpXQLVW2E/view?usp=sharing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgdown --id 1_8vmQwSZ02ZOMw_IPHY5OWkwpXQLVW2E # ID for Environmental Health data! look up!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m df_CalEnviroScreen \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCalEnviroScreen.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\surya\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\surya\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:457\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    456\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\surya\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1376\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1374\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1376\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\surya\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1250\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1248\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1253\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1254\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\surya\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CalEnviroScreen.xlsx'"
          ]
        }
      ],
      "source": [
        "# https://drive.google.com/file/d/1_8vmQwSZ02ZOMw_IPHY5OWkwpXQLVW2E/view?usp=sharing\n",
        "gdown --id 1_8vmQwSZ02ZOMw_IPHY5OWkwpXQLVW2E # ID for Environmental Health data! look up!\n",
        "df_CalEnviroScreen = pd.read_excel('CalEnviroScreen.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6STrxO8F3J1r",
        "outputId": "e9e088bf-be0b-4c42-a500-a7ee0cad6142"
      },
      "outputs": [],
      "source": [
        "# read data\n",
        "df = pd.read_excel('CalEnviroScreen.xlsx')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTanfsMdR0ys"
      },
      "source": [
        "Printing First 10 Records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "TShE_lFBR0FN",
        "outputId": "e40ee669-b5c9-4958-d0fa-857155afa37e"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXhCcHAgSUtM"
      },
      "source": [
        "Missing values across all the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54dSsYPSSUat",
        "outputId": "eb4d6934-96e6-466b-b282-20ba5c7a7db8"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpL0jio0Skzc"
      },
      "source": [
        "Total missing records with atleast one missing value are 2724"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQBxBryvShiN",
        "outputId": "d8f7124e-f3af-48f3-d42e-4b61e96451b8"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum().sum() # total records with at least one missing value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbTQazbPS7as"
      },
      "source": [
        "# Q2. Subsetting and Column info\n",
        "Simplify the raw dataframe so that you only keep the columns you need. The `X` variable will be the following columns: `Population`, `Ozone` through `Solid Waste Pctl`, and `Asthma` through `Linguistic Isolation Pctl`. The `y` variable will be `Poverty`. Examine the quality of each column and use your judgement about dropping rows or imputing missing values. Add text cells and lots of comments so we can understand your logic/justification!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMHP6DkgUOQP",
        "outputId": "20d954d7-c86c-483e-8baf-e53565d5a4ac"
      },
      "outputs": [],
      "source": [
        "#Getting columns names index\n",
        "column_name=df.columns\n",
        "print('Column Names:', pd.DataFrame(column_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qznw0dhsUsdd"
      },
      "source": [
        "Subsetting the dataset with desired columns. The `X` variable will be the following columns: `Population`, `Ozone` through `Solid Waste Pctl`, and `Asthma` through `Linguistic Isolation Pctl`. The `y` variable will be `Poverty`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP472a7g3sHS"
      },
      "source": [
        "## X and y dataset subsetting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KJtHHYZUcIP"
      },
      "outputs": [],
      "source": [
        "df1=df[df.columns[[1] + list(range(11, 35)) + [7]+ list(range(38,48))+[48]]] #Getting the required columns\n",
        "\n",
        "#subsetting the X from the df\n",
        "X=df1.drop('Poverty',axis=1)\n",
        "\n",
        "#Separating and Creating a y variable\n",
        "y=df1['Poverty']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5imlRKwx9fea",
        "outputId": "2667c64c-865b-4132-b36f-add7c56e0130"
      },
      "outputs": [],
      "source": [
        "X.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "kfKSXEwQ4gmy",
        "outputId": "a762d2d2-a12b-4f3a-f068-c940707038f1"
      },
      "outputs": [],
      "source": [
        "X.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAJq4uvL3y1N"
      },
      "source": [
        "## Missing Values (Desired Columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQDrFhyycgQC"
      },
      "source": [
        "Lets analyse the missing values in the desired columns.\n",
        "\n",
        "There aren't that many missing values. Total records having missing values are 1412, which is 17.5% of total records i.e. 8035. Here I have chosen to impute the records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4F4jZsLcjkK",
        "outputId": "091b3d93-941b-4a8c-e50d-958756516172"
      },
      "outputs": [],
      "source": [
        "X.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsTrOv7ELpN5",
        "outputId": "3d4b03fd-546e-470f-82f2-3d306bfff1ce"
      },
      "outputs": [],
      "source": [
        "1412/8035"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BpUTlVd335J"
      },
      "source": [
        "## KNN Imputation for Missing Values\n",
        "\n",
        "Here we will impute the missing values using KNN imputer. I have separated the columns that contains null values and also obtained the indices of it to verify that values have been imputed and indices are maintained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMIcOroZ57m6",
        "outputId": "0afa9cac-7c53-409f-cc35-8504800ac0fa"
      },
      "outputs": [],
      "source": [
        "#Getting the names of columns with NULL values\n",
        "null_columns_X = X.columns[X.isnull().any()]\n",
        "print('Names of Columns having Null values:/n', null_columns_X)\n",
        "\n",
        "# Getting the indices of null values\n",
        "null_indices = X[X.isnull().any(axis=1)].index\n",
        "\n",
        "# Displaying the top 5 null indices\n",
        "top_5_null_indices = null_indices[:5]\n",
        "print(\"/nTop 5 Null Indices:/n\")\n",
        "print(top_5_null_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "qVV9DXma7l0r",
        "outputId": "137734fe-2595-43b6-f19c-c8d0f6ae4351"
      },
      "outputs": [],
      "source": [
        "#Seeing the records before imputation\n",
        "X.loc[top_5_null_indices] #Notice the last two columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccbXWLOsC3d6"
      },
      "outputs": [],
      "source": [
        "#Imputing the missing values in the X\n",
        "#KNN imputation\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "knn_imputer = KNNImputer(n_neighbors=10,weights='distance')\n",
        "\n",
        "# Creating a new DataFrame with the selected columns (which has missing values)\n",
        "data_with_missing_values_X = X.loc[:, null_columns_X]\n",
        "\n",
        "# Imputing the missing values\n",
        "imputed_data_X = knn_imputer.fit_transform(data_with_missing_values_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "PcexQ05KufS9",
        "outputId": "33de14b7-1ac6-435f-89e4-c4ca5e145831"
      },
      "outputs": [],
      "source": [
        "# Creating a new DataFrame with the imputed values\n",
        "# setting the same index as X\n",
        "''' This is very important otherwise it will create its new index and\n",
        "create problem when we replace values in the original X dataset '''\n",
        "imputed_df_X = pd.DataFrame(imputed_data_X, columns=null_columns_X,index=X.index)\n",
        "\n",
        "# Replacing the imputed columns in the original DataFrame with the imputed values\n",
        "X[null_columns_X]=imputed_df_X\n",
        "\n",
        "#lets view the imputed data with same indices we saw earlier\n",
        "X.loc[top_5_null_indices]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeIgISfYTBv2"
      },
      "source": [
        "#Q3. Recoding Target Variable\n",
        "Recode the target variable to a 1 if greater than the mean value of poverty, otherwise make it a 0. Use this recoded variable as the target variable! Now it is a classification problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZBCq31AaP4w",
        "outputId": "ba0a56f1-0f5f-41f9-ae60-4108b9f4c4a9"
      },
      "outputs": [],
      "source": [
        "#Recoding the y variable to catergorical variable\n",
        "mean_poverty = y.mean()\n",
        "y= np.where(y > mean_poverty, 1, 0)\n",
        "\n",
        "#lets see the length of y\n",
        "len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew2sJEy6TFLd"
      },
      "source": [
        "#Q4. Plots and Tables\n",
        "Make two interesting plots or tables and a description of why you made the table and what you see.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWvsFDEGXgM5"
      },
      "source": [
        "## 1. Correlation of Variable with Target Variable (Original)\n",
        "\n",
        "Lets see how variable are correlated witht the target variable (the original one without recoding).\n",
        "\n",
        "The figure below indicates that ```CES 3.0 Score```and ```Asthama Pctl``` are significantly positively correlated to the ```Poverty```\n",
        "\n",
        "**Insights on relation to recoded values:**\n",
        "\n",
        "Since we have assigned 1 to ```Poverty``` greater than the mean of Poverty, the NN  model might assign weights to these two features such that the value of  ```Poverty``` number could increase; thereby, predicting 1.\n",
        "\n",
        "@Dave please correct me if I am wrong in giving a shot at (infering this!) what might be going inside the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "UJjWEFWAXGAA",
        "outputId": "d413ca3c-d8c0-4dfd-d3ab-bcbbd06d16dc"
      },
      "outputs": [],
      "source": [
        "# Calculate the correlation matrix between X_train features and y_train target variable\n",
        "corr_matrix = X.corrwith(df1['Poverty'])\n",
        "\n",
        "# Sort the correlation matrix in descending order\n",
        "sorted_corr_matrix = corr_matrix.sort_values(ascending=False)\n",
        "\n",
        "# Create a heatmap of the sorted correlation matrix\n",
        "fig, ax = plt.subplots(figsize=(6, 8))\n",
        "sns.heatmap(sorted_corr_matrix.to_frame(), annot=True, cmap='coolwarm', cbar=True, ax=ax)\n",
        "plt.title('Correlation Heatmap (Highest to Lowest)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUkIAIWgyQ7P"
      },
      "source": [
        "##2. Bar plot of Target Variable\n",
        "\n",
        "The count of the target variable (1 and 0) is plotted as a bar plot to better visualise if the dataset is balanced or imbalanced.\n",
        "\n",
        "Here from the graph we can tell that the dataset is nearly balanced.\n",
        "\n",
        "It is important to have the dataset balanced in order to predict both the classes without bias. In case of imbalanced data, the class with more data wou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "XWeDUUelZkoX",
        "outputId": "22f4ef3c-7de7-4c3e-b5a6-466f9ee7c314"
      },
      "outputs": [],
      "source": [
        "value_counts = np.bincount(y)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(range(len(value_counts)), value_counts)\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Count of Categories (1 and 0)')\n",
        "plt.xticks(range(len(value_counts)))\n",
        "for i, count in enumerate(value_counts):\n",
        "    plt.text(i, count, str(count), ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNMMRDQZ3eNB"
      },
      "source": [
        "##3. Correlation (all variables) Plot\n",
        "\n",
        "This plot gives an idea about what features are correlated with each other with what degree of correlation. It should be noted here that Pctl columns are percentile of its counterparts (example, PM2.5 has PM2.5 Pctl). It occured to me that as these are just repeated information in the data, could this be gotten rid of and then build a model. A thought behind this is keeping features which are same just coded differently might make mode more complex or might make it biased for some features. I will run a same model with these features removed towards the end of this collab just for the experiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "88TDyrTZAM_H",
        "outputId": "914a0b08-ac48-4c1a-f581-0ecd3656fa84"
      },
      "outputs": [],
      "source": [
        "# Calculating the correlation matrix for all features\n",
        "corr_matrix = X.corr()\n",
        "\n",
        "# Sort the correlation matrix in descending order\n",
        "sorted_corr_matrix = corr_matrix.unstack().sort_values(ascending=False)\n",
        "\n",
        "# Removing the correlation values with itself (diagonal elements)\n",
        "sorted_corr_matrix = sorted_corr_matrix[~(sorted_corr_matrix.index.get_level_values(0) == sorted_corr_matrix.index.get_level_values(1))]\n",
        "\n",
        "# Creating a heatmap of the sorted correlation matrix\n",
        "fig, ax = plt.subplots(figsize=(30, 30))\n",
        "sns.heatmap(sorted_corr_matrix.unstack(), annot=True, cmap='coolwarm', cbar=True, ax=ax, annot_kws={\"size\":16},fmt='.2f')\n",
        "plt.title('Correlation Heatmap (Highest to Lowest)',fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpvk-nfyTIQ0"
      },
      "source": [
        "#Q5. Spliiting Data\n",
        "Do an 90/10 split for X_train, X_test, y_train, y_test where the random seed is equal to your 7 digit studentID number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKXCmtrTzMw-"
      },
      "outputs": [],
      "source": [
        "#splitting dataset in train and test\n",
        "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.1, random_state=3069860)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syz8nj0hvdAT"
      },
      "source": [
        "Saving these split of train and test dataset in another variables X2 for other model that thought to experiment with (by removing the correlated features, mainly corresponding Pctl metric). In later part (in Appendix), the columns containng the Pctl were removed and same model (defined under different name) was run on this data to compare the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7_ODwt_OpfI"
      },
      "outputs": [],
      "source": [
        "#Saving these train and test dataset in another dataframe for another model (in Appendix)\n",
        "X2_train=X_train\n",
        "X2_test=X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG96oSWPb5w5",
        "outputId": "4109b38f-323d-4f2c-a3d6-ce70cefa923b"
      },
      "outputs": [],
      "source": [
        "#making sure that the data is split correctly\n",
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWl_3fIlTLc6"
      },
      "source": [
        "#Q6. Scaling\n",
        "Use the StandardScaler() on train and apply to test partition. Do not scale the target variable!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkrjQLCcz6cP"
      },
      "outputs": [],
      "source": [
        "standard_scaler = StandardScaler()\n",
        "X_train = standard_scaler.fit_transform(X_train)\n",
        "X_test = standard_scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8BydNASTP00"
      },
      "source": [
        "#Q7. Build and Compile the Model\n",
        "Build a model using the Sequential API (like we do in class) with at least 2 dense layers with the relu activation function, and with dropout in between each dense layer (use a number between 0.1 and 0.5). Compile the model using an appropriate optimizer. Use early stopping with patience of at least 10 and restore the best weights once the model converges. You can choose whatever batch size you would like to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHcrGwZz0h0H",
        "outputId": "d4a1d093-cd18-4828-c07a-6a1b16b6f84a"
      },
      "outputs": [],
      "source": [
        "X_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3opbSCdz0ja3",
        "outputId": "1ebc76f2-5c61-472e-c980-f6193fdc0d4f"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(X_train.shape[1],), activation='relu')) # (features,)\n",
        "model.add(Dropout(0.15)) # specify a percentage between 0 and 0.5\n",
        "model.add(Dense(64, activation='relu')) # output node\n",
        "model.add(Dropout(0.2)) # specify a percentage between 0 and 0.5\n",
        "model.add(Dense(32, activation='relu')) # output node\n",
        "model.add(Dropout(0.2)) # specify a percentage between 0 and 0.5\n",
        "model.add(Dense(1, activation='sigmoid')) # output node\n",
        "model.summary() # see what your model looks like\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O64Sz-By03Om"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diwWkbGpTSt0"
      },
      "source": [
        "#Q8. Fitting the Model\n",
        "Fit the model for 100000 epochs with a batch size of your choice, using X_test and y_test as the validation data. **Don’t forget the early stopping callback!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hj1g5LN1XDw",
        "outputId": "367934ca-d756-4175-f6e6-9bdb2a71fa67"
      },
      "outputs": [],
      "source": [
        "es = EarlyStopping(monitor='val_accuracy',\n",
        "                                   mode='max', # don't minimize the accuracy!\n",
        "                                   patience=20,\n",
        "                                   restore_best_weights=True)\n",
        "\n",
        "# now we just update our model fit call\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    callbacks=[es],\n",
        "                    epochs=100000, # you can set this to a big number!\n",
        "                    batch_size=256*2*2*2,\n",
        "                    validation_data=(X_test,y_test),\n",
        "                    shuffle=True,\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ustm8q5sTWGe"
      },
      "source": [
        "#Q9. Evaluating the Model\n",
        "Evaluate the model using learning curves, error metrics and confusion matrices for each partition (like we do in class). You should largely be able to copy and paste this from class notebooks. Add a few bullet points about what you see (did your model learn nice and gently?  If you don't have text cells here, you will lose points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "5-E3tvfh1gnW",
        "outputId": "484e072f-cda0-4c6c-d8db-a1290ec8f03a"
      },
      "outputs": [],
      "source": [
        "# learning curve\n",
        "\n",
        "# accuracy\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#loss\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaxFR2whtNfV"
      },
      "source": [
        "We can see in the learning curves that the model has pretty quickly learnt the pattern and loss has been reduced and accuracy has increased sharply.\n",
        "\n",
        "We can see from the confusion matrix that it has performed well on the train dataset achieving 0.95 F1 score. It also performs better on the test dataset with F1 score of 90. We have taken care of overfitting by dropout methods and it can be seen from F1 score of the train data that it has not overfit and predicts pretty good on test data. That being said, the accuracy on train is more than test, and also the loss on train is lesser than the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUSE2ETMTYWt"
      },
      "source": [
        "\n",
        "#Q10. Baseline Model and  Classification Report\n",
        "Calculate what a baseline prediction would be for the train and test partitions (a mean only model). Did your model do better than the baseline predictions? If so, you have a useful model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P_9_y6n10J4",
        "outputId": "cf985950-e044-4b44-b5de-45c50c30b084"
      },
      "outputs": [],
      "source": [
        "#Base line prediction for train\n",
        "y_train=pd.DataFrame(y_train)\n",
        "count_train=y_train.value_counts()\n",
        "\n",
        "baseline_pred_train=count_train[0]/(count_train[0]+count_train[1])\n",
        "print('Baseline prediction for train:/n', baseline_pred_train)\n",
        "\n",
        "#Base line prediction for test\n",
        "y_test=pd.DataFrame(y_test)\n",
        "count_test=y_test.value_counts()\n",
        "\n",
        "baseline_pred_test=count_train[0]/(count_train[0]+count_train[1])\n",
        "print('Baseline prediction for test:/n', baseline_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwzmOvMG5mBh",
        "outputId": "a2b39e7d-3508-4c3a-814a-730040c9b849"
      },
      "outputs": [],
      "source": [
        "## seeing how the model did! on train\n",
        "preds_train = np.round(model.predict(X_train),0)\n",
        "\n",
        "matrix = confusion_matrix(y_train,preds_train)\n",
        "matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiKXmmppJHyC",
        "outputId": "58d39544-f4a7-4d9b-e36b-45c601d1fa8c"
      },
      "outputs": [],
      "source": [
        "## seeing how the model did! on test\n",
        "preds_test = np.round(model.predict(X_test),0)\n",
        "\n",
        "matrix = confusion_matrix(y_test,preds_test)\n",
        "matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCvKiadNZ9Hq",
        "outputId": "65507d4d-1f1b-45a0-a11d-b562e704c90c"
      },
      "outputs": [],
      "source": [
        "#Printing the classification report on the train  data\n",
        "print('Classification Report on Train:\\n',classification_report(y_train , preds_train))\n",
        "\n",
        "#Printing the classification report on the test data\n",
        "print('Classification Report on Test:\\n',classification_report(y_test, preds_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeWyuww4UnDc"
      },
      "source": [
        "# Conclusion/Insights\n",
        "\n",
        "Extending the insgihts from model evaluation, following are some thoughts about this assignment\n",
        "\n",
        "1. After comparing the models (with same parameters; another model run in appendix below which did not include Pctl metric), It was observed that the first model (where the correlated features where present) had a tendency to overfit the train data (as the network got comple with more layers and units). The later model (where correlated features where removed), it gave pretty consistant performance of predicting train and test with similar numbers. The overfitting was not observed in later model (run in the appendix at the end).\n",
        "\n",
        "2. Also, it was observed that more the comlex model is (with more layers and units with dropouts), better was the performance of predictions in train and to some extent in test data.\n",
        "\n",
        "\n",
        "3. Compared to the baseline model, the F1 score of the model is 90 on test data. We can safely conclude that model performs better and has recognized patterns in the data to predict values.\n",
        "\n",
        "4. In both the models (above and in the appendix), the validation accuracy is less than the training. Also, the validation loss is more (as in the loss number) than the training data.\n",
        "\n",
        "5. The first model (where correlated features such as Pctl is present) has 36 total number of features. Whereas, after removing the correlated features (for which count is 17), the total number of feature in model_2 are 19. This might explain the consistency of the model between the train and test data. In that, the model is not fed with lot of noise. (@Dave please correct me if this inference is wrong; I would love to know more and learn from your feedback)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZRnYEeZ6RNR"
      },
      "source": [
        "##**Appendix (Other model)**\n",
        "\n",
        "Checking the same model performance when we get rid of the similar metric called Pctl against some of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "p87N4ssT6Vbi",
        "outputId": "812d909d-25c2-4950-95c0-de820cf6c754"
      },
      "outputs": [],
      "source": [
        "#selecting columnns without Pctl metric\n",
        "X_columns_wo_Pctl = [col for col in X.columns if \"Pctl\" not in col]\n",
        "X2_train=X2_train[X_columns_wo_Pctl]\n",
        "X2_test=X2_test[X_columns_wo_Pctl]\n",
        "#lets view the 5 records\n",
        "X2_train.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic72J5pf6Vbk"
      },
      "source": [
        "### Scaling\n",
        "Use the StandardScaler() on train and apply to test partition. Do not scale the target variable!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q8AVlIE6Vbk"
      },
      "outputs": [],
      "source": [
        "standard_scaler = StandardScaler()\n",
        "X2_train = standard_scaler.fit_transform(X2_train)\n",
        "X2_test = standard_scaler.transform(X2_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48J2uPMF6Vbk"
      },
      "source": [
        "### Build and Compile the Model\n",
        "Build a model using the Sequential API (like we do in class) with at least 2 dense layers with the relu activation function, and with dropout in between each dense layer (use a number between 0.1 and 0.5). Compile the model using an appropriate optimizer. Use early stopping with patience of at least 10 and restore the best weights once the model converges. You can choose whatever batch size you would like to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVeuWawX6Vbk",
        "outputId": "730b05a5-f7fe-4ca9-d1d0-44a5d8e159a9"
      },
      "outputs": [],
      "source": [
        "X2_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqaD-ZT86Vbk",
        "outputId": "a3e8c272-8343-4ae4-c78f-d15fc1bd130b"
      },
      "outputs": [],
      "source": [
        "model_2 = Sequential()\n",
        "model_2.add(Dense(128, input_shape=(X2_train.shape[1],), activation='relu')) # (features,)\n",
        "model_2.add(Dropout(0.15)) # specify a percentage between 0 and 0.5\n",
        "model_2.add(Dense(64, activation='relu')) # output node\n",
        "model_2.add(Dropout(0.2)) # specify a percentage between 0 and 0.5\n",
        "model_2.add(Dense(32, activation='relu')) # output node\n",
        "model_2.add(Dropout(0.2)) # specify a percentage between 0 and 0.5\n",
        "model_2.add(Dense(1, activation='sigmoid')) # output node\n",
        "model_2.summary() # see what your model looks like\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqJaZMq96Vbk"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "model_2.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox5gVH0s6Vbk"
      },
      "source": [
        "### Fitting the Model\n",
        "Fit the model for 100000 epochs with a batch size of your choice, using X_test and y_test as the validation data. **Don’t forget the early stopping callback!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EDMj5016Vbk",
        "outputId": "7ba07f3b-d4b9-4882-a681-bcc0d85e538c"
      },
      "outputs": [],
      "source": [
        "es = EarlyStopping(monitor='val_accuracy',\n",
        "                                   mode='max', # don't minimize the accuracy!\n",
        "                                   patience=20,\n",
        "                                   restore_best_weights=True)\n",
        "\n",
        "# now we just update our model fit call\n",
        "history_2 = model_2.fit(X2_train,\n",
        "                    y_train,\n",
        "                    callbacks=[es],\n",
        "                    epochs=100000, # you can set this to a big number!\n",
        "                    batch_size=256*2*2*2,\n",
        "                    validation_data=(X2_test,y_test),\n",
        "                    shuffle=True,\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyObpvg86Vbk"
      },
      "source": [
        "###Evaluating the Model\n",
        "Evaluate the model using learning curves, error metrics and confusion matrices for each partition (like we do in class). You should largely be able to copy and paste this from class notebooks. Add a few bullet points about what you see (did your model learn nice and gently?  If you don't have text cells here, you will lose points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "POEfsaOb6Vbk",
        "outputId": "6c691335-6f7a-42f3-ce35-cf06dc8f66cf"
      },
      "outputs": [],
      "source": [
        "# learning curve\n",
        "\n",
        "# accuracy\n",
        "acc_2 = history_2.history['accuracy']\n",
        "val_acc_2 = history_2.history['val_accuracy']\n",
        "\n",
        "\n",
        "epochs = range(1, len(acc_2) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, acc_2, 'bo', label='Training accuracy')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_acc_2, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#loss\n",
        "loss_2 = history_2.history['loss']\n",
        "val_loss_2 = history_2.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc_2) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss_2, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss_2, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "251IL5O76Vbl"
      },
      "source": [
        "\n",
        "### Baseline Model and Evaluation on Test Data\n",
        "Calculate what a baseline prediction would be for the train and test partitions (a mean only model). Did your model do better than the baseline predictions? If so, you have a useful model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC_O8jka6Vbl",
        "outputId": "86d28c3e-cc3e-469b-b63d-8d96107d687e"
      },
      "outputs": [],
      "source": [
        "#Base line prediction for train\n",
        "y_train=pd.DataFrame(y_train)\n",
        "count_train=y_train.value_counts()\n",
        "\n",
        "baseline_pred_train=count_train[0]/(count_train[0]+count_train[1])\n",
        "print('Baseline prediction for train:/n', baseline_pred_train)\n",
        "\n",
        "#Base line prediction for test\n",
        "y_test=pd.DataFrame(y_test)\n",
        "count_test=y_test.value_counts()\n",
        "\n",
        "baseline_pred_test=count_train[0]/(count_train[0]+count_train[1])\n",
        "print('Baseline prediction for test:/n', baseline_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0YqzaPP6Vbl",
        "outputId": "a6ab5b3c-6b4b-4bbf-db3d-c1282daf0d07"
      },
      "outputs": [],
      "source": [
        "## seeing how the model did! on train\n",
        "preds2_train = np.round(model_2.predict(X2_train),0)\n",
        "\n",
        "matrix = confusion_matrix(y_train,preds2_train)\n",
        "matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIxh7KQM6Vbl",
        "outputId": "8438f202-fdbc-4ce0-ace3-80b3e7768089"
      },
      "outputs": [],
      "source": [
        "## seeing how the model did! on test\n",
        "preds2_test = np.round(model_2.predict(X2_test),0)\n",
        "\n",
        "matrix = confusion_matrix(y_test,preds2_test)\n",
        "matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxeP1lOY6Vbl",
        "outputId": "3207ac9f-fb9a-4d1f-dac0-c3003afbba21"
      },
      "outputs": [],
      "source": [
        "#Printing the classification report on the train  data\n",
        "print('Classification Report on Train:\\n',classification_report(y_train , preds2_train))\n",
        "\n",
        "#Printing the classification report on the test data\n",
        "print('Classification Report on Test:\\n',classification_report(y_test, preds2_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
